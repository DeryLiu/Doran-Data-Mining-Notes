[面试题来源](https://developer.aliyun.com/ask/257628?utm_content=g_1000088312)
- 异常值是指什么？请列举1种识别连续型变量异常值的方法？
```text
1. 异常值定义：
异常值(Outlier)是指样本中的个别值，其数值明显偏离所属样本的其余观测值。
在数理统计里一般是指一组观测值中与平均值的偏差超过两倍标准差的测定值。

2. 识别连续型变量异常值的方法：
Grubbs’ test(是以Frank E. Grubbs命名的)，又叫maximum normed residual test，
是一种用于单变量数据集异常值识别的统计检测，它假定数据集来自正态分布的总体。

未知总体标准差σ，在五种检验法中，优劣次序为：t检验法、格拉布斯检验法、峰度检验法、狄克逊检验法、偏度检验法。
```

- 什么是聚类分析？
```text
聚类分析(cluster analysis)是一组将研究对象分为相对同质的群组(clusters)的统计分析技术。
聚类分析也叫分类分析(classification analysis)或数值分类(numerical taxonomy)。
聚类与分类的不同在于，聚类所要求划分的类是未知的。
```

- 聚类算法有哪几种？选择一种详细描述其计算原理和步骤？
```text
聚类分析计算方法主要有：
层次的方法(hierarchical method)、划分方法(partitioning method)、基于密度的方法(density-based method)、基于网格的方法(grid-based method)、基于模型的方法(model-based method)等。
其中，前两种算法是利用统计学定义的距离进行度量。

k-means 算法的工作过程说明如下：首先从n个数据对象任意选择 k 个对象作为初始聚类中心;而对于所剩下其它对象，则根据它们与这些聚类中心的相似度(距离)，分别将它们分配给与其最相似的(聚类中心所代表的)聚类;然 后再计算每个所获新聚类的聚类中心(该聚类中所有对象的均值);不断重复这一过程直到标准测度函数开始收敛为止。一般都采用均方差作为标准测度函数. k个聚类具有以下特点：各聚类本身尽可能的紧凑，而各聚类之间尽可能的分开。

其流程如下：
(1)从 n个数据对象任意选择 k 个对象作为初始聚类中心;
(2)根据每个聚类对象的均值(中心对象)，计算每个对象与这些中心对象的距离;并根据最小距离重新对相应对象进行划分;
(3)重新计算每个(有变化)聚类的均值(中心对象);
(4)循环(2)、(3)直到每个聚类不再发生变化为止(标准测量函数收敛)。
优点： 本算法确定的K 个划分到达平方误差最小。当聚类是密集的，且类与类之间区别明显时，效果较好。对于处理大数据集，这个算法是相对可伸缩和高效的，计算的复杂度为 O(NKt)，其中N是数据对象的数目，t是迭代的次数。一般来说，K<
缺点： 1. K 是事先给定的，但非常难以选定;2. 初始聚类中心的选择对聚类结果有较大的影响。
```

- 什么是数据标准化，为什么要进行数据标准化？
```text
数据标准化是预处理步骤，将数据标准化到一个特定的范围能够在反向传播中保证更好的收敛。
一般来说，是将该值将去平均值后再除以标准差。
如果不进行数据标准化，有些特征（值很大）将会对损失函数影响更大（就算这个特别大的特征只是改变了1%，但是他对损失函数的影响还是很大，并会使得其他值比较小的特征变得不重要了）。
因此数据标准化可以使得每个特征的重要性更加均衡。
```

- 如何处理缺失值数据？
```text
数据的缺失值处理主要依赖于该数据的特征对模型的影响，处理的方法有两种，一种是删除整行或者整列的数据，另一种则是使用其他值去填充这些缺失值，比如数值型的特征可以选择去除、平均值以及线性插值的方式。
具体哪种处理方式更为有效取决于你自己的业务和模型。
所以一般在处理完数据后会对数据集进行划分为训练集、验证集、测试集，然后训练并查看结果。
```

- 如何进行探索性数据分析(EDA)？
```text
EDA的目的是去挖掘数据的一些重要信息。一般情况下会从粗到细的方式进行EDA探索。
一开始我们可以去探索一些全局性的信息，观察一些不平衡的数据，计算一下各个类的方差和均值。
看一下前几行数据的信息，包含什么特征等信息。使用Pandas中的df.info()去了解哪些特征是连续的，离散的，它们的类型(int、float、string)。
接下来，删除一些不需要的列，这些列就是那些在分析和预测的过程中没有什么用的。

比如：某些列的值很多都是相同的，或者这些列有很多缺失值。当然你也可以去用一些中位数等去填充这些缺失值。然后我们可以去做一些可视化。对于一些类别特征或者值比较少的可以使用条形图。类标和样本数的条形图。找到一些最一般的特征。对一些特征和类别的关系进行可视化去获得一些基本的信息。然后还可以可视化两个特征或三个特征之间的关系，探索特征之间的联系。

你也可以使用PCA去了解哪些特征更加重要。组合特征去探索他们的关系，比如当A=0，B=0的类别是什么，A=1，B=0呢？比较特征的不同值，比如性别特征有男女两个取值，我们可以看下男和女两种取值的样本类标会不会不一样。

另外，除了条形图、散点图等基本的画图方式外，也可以使用PDF\CDF或者覆盖图等。观察一些统计数据比如数据分布、p值等。这些分析后，最后就可以开始建模了。

一开始可以使用一些比较简单的模型比如贝叶斯模型和逻辑斯谛回归模型。如果你发现你的数据是高度非线性的，你可以使用多项式回归、决策树或者SVM等。特征选择则可以基于这些特征在EDA过程中分析的重要性。如果你的数据量很大的话也可以使用神经网络。然后观察ROC曲线、查全率和查准率。
```

- 在图像处理中为什么要使用卷积神经网络而不是全连接网络？
```text
首先，卷积过程是考虑到图像的局部特征，能够更加准确的抽取空间特征。如果使用全连接的话，我们可能会考虑到很多不相关的信息。
其次，CNN有平移不变性，因为权值共享，图像平移了，卷积核还是可以识别出来，但是全连接则做不到。
```

- 是什么使得CNN具有平移不变性？
```text
每个卷积核都是一个特征探测器。所以就像我们在侦查一样东西的时候，不管物体在图像的哪个位置都能识别该物体。
因为在卷积过程，我们使用卷积核在整张图片上进行滑动卷积，所以CNN具有平移不变性。
```

- 为什么实现分类的CNN中需要进行Max-pooling？
```text
Max-pooling可以将特征维度变小，使得减小计算时间，同时，不会损失太多重要的信息，因为我们是保存最大值，这个最大值可以理解为该窗口下的最重要信息。
同时，Max-pooling也对CNN具有平移不变性提供了很多理论支撑。
```

- 为什么应用于图像切割的CNN一般都具有Encoder-Decoder架构？
```text
Encoder CNN一般被认为是进行特征提取，而decoder部分则使用提取的特征信息并且通过decoder这些特征和将图像缩放到原始图像大小的方式去进行图像切割。
```

- 什么是batch normalization，原理是什么？
```text
Batch Normalization就是在训练过程，每一层输入加一个标准化处理。

深度神经网络之所以复杂有一个原因就是由于在训练的过程中上一层参数的更新使得每一层的输入一直在改变。
所以有个办法就是去标准化每一层的输入。具体归一化的方式如下图，如果只将归一化的结果进行下一层的输入，这样可能会影响到本层学习的特征，因为可能该层学习到的特征分布可能并不是正态分布的，这样强制变成正态分布会有一定影响，所以还需要乘上γ和β，这两个参数是在训练过程学习的，这样可以保留学习到的特征。
```

- 为什么卷积核一般都是3*3而不是更大？
```text
主要有这2点原因：
第一：相对于用较大的卷积核，使用多个较小的卷积核可以获得相同的感受野和能获得更多的特征信息，同时使用小的卷积核参数更少，计算量更小。
第二：你可以使用更多的激活函数，有更多的非线性，使得在你的CNN模型中的判决函数有更有判决性。
```

- 用户调研设计？
  - 某公司针对A、B、C三类客户，提出了一种统一的改进计划，用于提升客户的周消费次数，需要你来制定一个事前试验方案，来支持决策，请你思考下列问题：
    - a) 试验需要为决策提供什么样的信息？
    - c) 按照上述目的，请写出你的数据抽样方法、需要采集的数据指标项，以及你选择的统计方法。
    - a) 试验要能证明该改进计划能显著提升A、B、C三类客户的周消费次数。
    - b) 根据三类客户的数量，采用分层比例抽样;

```text
需要采集的数据指标项有：客户类别，改进计划前周消费次数，改进计划后周消费次数;
选用统计方法为：分别针对A、B、C三类客户，进行改进前和后的周消费次数的，两独立样本T-检验(two-sample t-test)。
```
